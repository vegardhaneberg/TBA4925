{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from calendar import monthrange, month_name\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import cdsapi\n",
    "import math\n",
    "\n",
    "# Interpolation\n",
    "from scipy.interpolate import RegularGridInterpolator, LinearNDInterpolator\n",
    "import scipy.interpolate.interpnd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data frame options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_cygnss_boolean = False\n",
    "load_smap_boolean = True\n",
    "interpolate_dfs_boolean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE CYGNSS DATAFRAME ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sr_value(snr, p_r, g_t, g_r, d_ts, d_sr):\n",
    "    # snr(dB), p_r(dBW), g_t(dBi), g_r(dBi), d_ts(meter), d_sr(meter)\n",
    "    return snr - p_r - g_t - g_r - (20 * np.log10(0.19)) + (20 * np.log10(d_ts + d_sr)) + (20 * np.log10(4 * np.pi))\n",
    "\n",
    "\n",
    "def compute_surface_reflectivity(df):\n",
    "    df['sr'] = df.apply(\n",
    "        lambda row: calculate_sr_value(row.ddm_snr, row.gps_tx_power_db_w, row.gps_ant_gain_db_i, row.sp_rx_gain,\n",
    "                                       row.tx_to_sp_range, row.rx_to_sp_range), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_hours_after_jan_value(day_of_year, ddm_timestamp):\n",
    "    return (day_of_year - 1) * 24 + round(ddm_timestamp / (60 * 60))\n",
    "\n",
    "\n",
    "def compute_hours_after_jan(df):\n",
    "    df['hours_after_jan_2020'] = df.apply(\n",
    "        lambda row: calculate_hours_after_jan_value(row.day_of_year, row.ddm_timestamp_utc), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_unique_track_id_value(track_id, day_of_year, prn_nr, sat_nr):\n",
    "    return track_id * 10000 + prn_nr * 10 + sat_nr + day_of_year/1000\n",
    "\n",
    "\n",
    "def compute_unique_track_ids(df):\n",
    "    df['unique_track_id'] = df.apply(\n",
    "        lambda row: generate_unique_track_id_value(row.track_id, row.day_of_year, row.prn_code, row.spacecraft_num), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_qf_list(qf_number):\n",
    "    qf_list = []\n",
    "    binary = format(qf_number, 'b')\n",
    "    for i in range(len(binary)):\n",
    "        if binary[i] == '1':\n",
    "            qf_list.append(2 ** (int(i)))\n",
    "\n",
    "    return qf_list\n",
    "\n",
    "\n",
    "def compute_prn_to_block_value(prn_code):\n",
    "    iir_list = [2, 13, 16, 19, 20, 21, 22]\n",
    "    iif_list = [1, 3, 6, 8, 9, 10, 25, 26, 27, 30, 32]\n",
    "    iir_m_list = [5, 7, 12, 15, 17, 29, 31]\n",
    "    iii_list = [4, 11, 14, 18, 23, 24]\n",
    "    \n",
    "    if prn_code in iir_list:\n",
    "        return 'IIR'\n",
    "    elif prn_code in iif_list:\n",
    "        return 'IIF'\n",
    "    elif prn_code in iir_m_list:\n",
    "        return 'IIR-M'\n",
    "    elif prn_code in iii_list:\n",
    "        return 'III'\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "\n",
    "def compute_block_code(df):\n",
    "    df['block_code'] = df.apply(lambda row: compute_prn_to_block_value(row.prn_code), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_daily_hour_column(df):\n",
    "    df['daily_hour'] = df.apply(lambda row: round(row.ddm_timestamp_utc / (60*60)), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_time_of_day_value(time):\n",
    "    if time >= 22:\n",
    "        return 'N'\n",
    "    elif time >= 16:\n",
    "        return 'A'\n",
    "    elif time >= 10:\n",
    "        return 'D'\n",
    "    elif time >= 4:\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'N'\n",
    "    \n",
    "\n",
    "def compute_time_of_day(df):\n",
    "    df['time_of_day'] = df.apply(lambda row: compute_time_of_day_value(row.daily_hour), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_sr_values(df):\n",
    "    min_sr = df['sr'].min()\n",
    "    df['sr'] = df['sr'].apply(lambda x: x - min_sr)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_location(df, location, cygnss=True):\n",
    "    if cygnss:\n",
    "        filtered_df = df[df.sp_lat < location[0]]\n",
    "        filtered_df = filtered_df[filtered_df.sp_lat > location[2]]\n",
    "        filtered_df = filtered_df[filtered_df.sp_lon < location[3]]\n",
    "        filtered_df = filtered_df[filtered_df.sp_lon > location[1]]\n",
    "    else:\n",
    "        filtered_df = df[df.lat < location[0]]\n",
    "        filtered_df = filtered_df[filtered_df.lat > location[2]]\n",
    "        filtered_df = filtered_df[filtered_df.long < location[3]]\n",
    "        filtered_df = filtered_df[filtered_df.long > location[1]]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def filter_quality_flags_1(df):\n",
    "    df['qf_ok'] = df.apply(\n",
    "        lambda row: (2 or 4 or 5 or 8 or 16 or 17) not in generate_qf_list(int(row.quality_flags)), axis=1)\n",
    "    df = df[df['qf_ok']]\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_quality_flags_2(df):\n",
    "    res_df = df\n",
    "    res_df['qf2_ok'] = res_df.apply(\n",
    "        lambda row: (1 or 2) not in generate_qf_list(int(row.quality_flags_2)), axis=1)  # Remember to check which qfs\n",
    "    res_df = res_df[res_df['qf2_ok']]\n",
    "    return res_df\n",
    "\n",
    "def filter_cygnss_qf(df):\n",
    "    remove_mask = int('11000000010011010', 2)\n",
    "\n",
    "    return df[df['quality_flags'] & remove_mask == 0]\n",
    "\n",
    "\n",
    "def remove_fill_values(df, raw_data):\n",
    "    keys = list(raw_data.keys())\n",
    "    keys.remove('ddm_timestamp_utc')\n",
    "    keys.remove('spacecraft_num')\n",
    "    filtered_df = df\n",
    "\n",
    "    # Remove rows containing fill values\n",
    "    for k in keys:\n",
    "        key = raw_data[k]\n",
    "        fv = key._FillValue\n",
    "        filtered_df = filtered_df[filtered_df[k] != fv]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Returning the same df if no specific features are selected\n",
    "def select_df_features(df, feature_list):\n",
    "    if len(feature_list) > 0:\n",
    "        return df[feature_list]\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def store_df_as_csv(df, storage_path, file_name):\n",
    "    storage_dir = Path(storage_path)\n",
    "    storage_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(storage_dir / file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PROCESSING MAIN FUNCTIONS ###\n",
    "def raw_df_processing(df, location, qf1_removal=False, qf2_removal=False):\n",
    "    res_df = df\n",
    "\n",
    "    #print('Filtering the DataFrame based on provided location...')\n",
    "    res_df = filter_location(res_df, location)\n",
    "\n",
    "    if qf1_removal:\n",
    "        print('Removing bad quality values...')\n",
    "        rows_before_removal = res_df.shape[0]\n",
    "        #res_df = filter_quality_flags_1(res_df)\n",
    "        res_df = filter_cygnss_qf(res_df)\n",
    "        rows_after_removal = res_df.shape[0]\n",
    "        print('Removed ' + str(rows_before_removal - rows_after_removal) + ' rows of bad overall quality...')\n",
    "\n",
    "    if qf2_removal:\n",
    "        print('Removing more bad quality values...')\n",
    "        rows_before_removal = res_df.shape[0]\n",
    "        res_df = filter_quality_flags_2(res_df)\n",
    "        rows_after_removal = res_df.shape[0]\n",
    "        print('Removed ' + str(rows_before_removal - rows_after_removal) + ' rows of bad overall quality...')\n",
    "\n",
    "    #print('Computing surface reflectivity values for all rows...')\n",
    "    res_df = compute_surface_reflectivity(res_df)\n",
    "\n",
    "    #print('Adding column displaying hours after January 1st 2020...')\n",
    "    res_df = compute_hours_after_jan(res_df)\n",
    "\n",
    "    #print('Computing unique track ids for all rows...')\n",
    "    res_df = compute_unique_track_ids(res_df)\n",
    "\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def process_monthly_df(retrieval_folder, year, month, location, qf1_removal=True, qf2_removal=False):\n",
    "    monthly_df = pd.DataFrame()\n",
    "    num_of_days = monthrange(year, month)[1]\n",
    "\n",
    "    for i in range(num_of_days):\n",
    "        csv_path = retrieval_folder + str(month).zfill(2) + '/raw_main_df_' + str(year) + '_' + str(month).zfill(2) + '_' + \\\n",
    "                   str(i + 1) + 'of' + str(num_of_days) + '.csv'\n",
    "        #print('#######################################')\n",
    "        #print('Collecting csv file number ' + str(i + 1) + ' of ' + str(num_of_days) + '...')\n",
    "        daily_df = pd.read_csv(csv_path)\n",
    "        #print('***Processing the data***')\n",
    "        daily_df = raw_df_processing(daily_df, location, qf1_removal, qf2_removal)\n",
    "        monthly_df = monthly_df.append(daily_df, ignore_index=True)\n",
    "        #print('#######################################\\n')\n",
    "    return monthly_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cygnss_df(location, year, start_month=1, end_month=12):\n",
    "    location_string = str(location[0]) + '-' + str(location[1]) + '-' + str(location[2]) + '-' + str(location[3])\n",
    "    yearly_cygnss_df = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(start_month, end_month+1)):\n",
    "        print('*' * 39)\n",
    "        print('*' * 14 + f'{month_name[i]:^11}' + '*' * 14)\n",
    "        print('*' * 39 + '\\n')\n",
    "        monthly_df = process_monthly_df(cygnss_retrieval_folder, year, i, location, False, False)\n",
    "        yearly_cygnss_df = yearly_cygnss_df.append(monthly_df, ignore_index=True)\n",
    "    \n",
    "    file_name = 'CYGNSS' + str(year) + '-withQFs-[' + location_string + '].csv'\n",
    "    store_df_as_csv(yearly_cygnss_df, processed_cygnss_storage_folder, file_name)\n",
    "    return yearly_cygnss_df\n",
    "\n",
    "\n",
    "def get_cygnss_df(location, year):\n",
    "    location_string = str(location[0]) + '-' + str(location[1]) + '-' + str(location[2]) + '-' + str(location[3])\n",
    "    file_name = 'CYGNSS' + str(year) + '-withQFs-[' + location_string + '].csv'\n",
    "    \n",
    "    try:\n",
    "        return pd.read_csv('/Users/madsrindal/Desktop/Intervals/' + location_string + '/' + file_name)\n",
    "        #return pd.read_csv('/Volumes/MadsDrive/Master/Processed Files/' + location_string + '/' + file_name)\n",
    "    except:\n",
    "        return process_cygnss_df(location, year)\n",
    "        #return process_cygnss_df(location, year, start_month=11, end_month=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "process_single_area = True\n",
    "process_single_year = False\n",
    "\n",
    "#single_area = 'WestIranFlood2020'\n",
    "#single_location = [35, 49.5, 30, 54.5]\n",
    "\n",
    "\n",
    "single_area = 'CentralAfrica'\n",
    "single_location = [-7, 23, -12, 28]\n",
    "\n",
    "\n",
    "if process_single_year:\n",
    "    years = [single_year]\n",
    "else:\n",
    "    years = [2019, 2020, 2021]\n",
    "\n",
    "if process_single_area:\n",
    "    all_areas = [single_area]\n",
    "    all_locations = [single_location]\n",
    "else:\n",
    "    all_areas = ['CentralAfrica', 'Brazil', 'Australia', 'IranPakistan', 'India']\n",
    "    all_locations = [[-7, 23, -12, 28], [-5, -42, -10, -37], [-22, 117, -27, 122], [31, 59, 26, 64], [24, 80, 19, 85]]\n",
    "\n",
    "locations_df = pd.DataFrame.from_dict({'area': all_areas, 'location': all_locations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if load_cygnss_boolean:\n",
    "    \n",
    "    for desired_year in years:\n",
    "        cygnss_retrieval_folder = '/Volumes/DACOTA HDD/Semester Project CSV/CYGNSS ' + str(desired_year) + '-'\n",
    "\n",
    "        for loc in locations_df['location']:\n",
    "            location_string = str(loc[0]) + '-' + str(loc[1]) + '-' + str(loc[2]) + '-' + str(loc[3])\n",
    "            #processed_cygnss_storage_folder = '/Volumes/MadsDrive/Master/Processed Files/' + location_string\n",
    "            processed_cygnss_storage_folder = '/Users/madsrindal/Desktop/Intervals/' + location_string\n",
    "            get_cygnss_df(loc, desired_year)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD SMAP DATAFRAME ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INTERPOLATION FUNCTIONS ###\n",
    "\n",
    "def interpolate_ml(df: pd.DataFrame, target_value='swvl1') -> LinearNDInterpolator:\n",
    "    coordinates = list(zip(list(df['time']), list(df['lat']), list(df['long'])))\n",
    "    target = df[target_value]\n",
    "    interpolation_function = LinearNDInterpolator(coordinates, target)\n",
    "    return interpolation_function\n",
    "\n",
    "\n",
    "def filter_nan_smap_sm(df):\n",
    "    try:\n",
    "        df['smap_sm'] = df['smap_sm'].apply(lambda x: x.item(0))\n",
    "    except:\n",
    "        print('SMAP_SM value was already of type: float')\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_nan_smap_vo(df):\n",
    "    try:\n",
    "        df['smap_vo'] = df['smap_vo'].apply(lambda x: x.item(0))\n",
    "    except:\n",
    "        print('SMAP_VO value was already of type: float')\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_nan_smap_sr(df):\n",
    "    try:\n",
    "        df['smap_surface_roughness'] = df['smap_surface_roughness'].apply(lambda x: x.item(0))\n",
    "    except:\n",
    "        print('SMAP_Surface_Roughness value was already of type: float')\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_smap(path: str, printing=False):\n",
    "\n",
    "    ds = nc.Dataset(path)\n",
    "    sm = ds['Soil_Moisture_Retrieval_Data_AM']\n",
    "\n",
    "    lats = np.array(sm['latitude']).flatten()\n",
    "    lons = np.array(sm['longitude']).flatten()\n",
    "    times = np.array(sm['tb_time_utc']).flatten()\n",
    "    sms = np.array(sm['soil_moisture']).flatten()\n",
    "    qfs = np.array(sm['retrieval_qual_flag']).flatten()\n",
    "    vos = np.array(sm['vegetation_opacity']).flatten()\n",
    "    srs = np.array(sm['roughness_coefficient']).flatten()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['lat'] = lats\n",
    "    df['long'] = lons\n",
    "    df['time'] = times\n",
    "    df['smap_sm'] = sms\n",
    "    df['retrieval_qfs'] = qfs\n",
    "    df['surface_roughness'] = srs\n",
    "    df['vegetation_opacity'] = vos\n",
    "\n",
    "    # Filter out missing values\n",
    "    smap_df = df[df['smap_sm'] != -9999.0]\n",
    "\n",
    "    if len(smap_df) > 0 and printing:\n",
    "        print('Number of missing values:', len(df) - len(smap_df))\n",
    "        print('Number of data points with value:', len(smap_df))\n",
    "        index = list(smap_df['smap_sm']).index(max(list(smap_df['smap_sm'])))\n",
    "        print(\"Peak SM value:\", list(smap_df['smap_sm'])[index])\n",
    "        print(\"Peak SM value at: (\" + str(list(smap_df['lat'])[index]) + \", \" + str(list(smap_df['long'])[index]) + \")\")\n",
    "\n",
    "    return smap_df\n",
    "\n",
    "\n",
    "def conv(t):\n",
    "    try:\n",
    "        return pd.Timestamp(t)\n",
    "    except:\n",
    "        return pd.Timestamp(t.split('.')[0] + '.000Z')\n",
    "\n",
    "\n",
    "def convert_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ref_date = pd.Timestamp('2019-01-01T00:00:00.000Z')\n",
    "\n",
    "    df['time'] = df['time'].apply(lambda t: conv(t))\n",
    "    df['time'] = df['time'].apply(lambda t: (t - ref_date).days * 24 + (t - ref_date).seconds / 3600)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_smap_df_year(root_dir: str, year: int, convert_time_hours=True) -> pd.DataFrame:\n",
    "    first = True\n",
    "    all_paths = []\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if not first:\n",
    "                all_paths.append(os.path.join(subdir, file))\n",
    "            else:\n",
    "                first = False\n",
    "\n",
    "    smap_df = pd.DataFrame()\n",
    "\n",
    "    for path in tqdm(all_paths):\n",
    "        path_split = path.split('_')\n",
    "        #print(path_split)\n",
    "        if len(path_split) > 6:\n",
    "            current_year = int(path_split[4][:4])\n",
    "\n",
    "            if current_year == year:\n",
    "                current_df = get_smap(path)\n",
    "                smap_df = smap_df.append(current_df)\n",
    "\n",
    "    if convert_time_hours:\n",
    "        smap_df = convert_time(smap_df)\n",
    "\n",
    "    return smap_df\n",
    "\n",
    "\n",
    "def get_smap_main(root_path: str, years: list, months: list, days: list) -> pd.DataFrame:\n",
    "    first = True\n",
    "    sub_dirs = []\n",
    "    filenames = []\n",
    "\n",
    "    for dir_name, sub_dir_list, file_list in os.walk(root_path):\n",
    "        if first:\n",
    "            sub_dirs = sub_dir_list\n",
    "            first = False\n",
    "        else:\n",
    "            filenames.append(file_list[0])\n",
    "    \n",
    "    smap_df = pd.DataFrame()\n",
    "    \n",
    "    for i in tqdm(range(len(sub_dirs))):\n",
    "        current_day = int(filenames[i].split('_')[4][6:8])\n",
    "        current_month = int(filenames[i].split('_')[4][4:6])\n",
    "        current_year = int(filenames[i].split('_')[4][:4])\n",
    "        \n",
    "        if (current_day in days) and (current_year in years) and (current_month in months):\n",
    "            current_path = root_path + '/' + sub_dirs[i] + '/' + filenames[i]\n",
    "            current_df = get_smap(current_path)\n",
    "            smap_df = smap_df.append(current_df)\n",
    "    \n",
    "    smap_df = convert_time(smap_df)\n",
    "    return smap_df\n",
    "\n",
    "\n",
    "def interpolate_smap_df(cygnss_df, interpolation_function, column_name):\n",
    "    cygnss_df[column_name] = cygnss_df.progress_apply(lambda row: interpolation_function(row.hours_after_jan_2019, row.sp_lat, row.sp_lon), axis=1)\n",
    "    return cygnss_df\n",
    "\n",
    "\n",
    "def filter_smap_qfs(smap_df):\n",
    "    return smap_df.loc[(smap_df['retrieval_qfs'] == 0) | (smap_df['retrieval_qfs'] == 8)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [02:58<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "if load_smap_boolean:\n",
    "    \n",
    "    for area in locations_df['area']:\n",
    "        loc = list(locations_df['location'][locations_df['area'] == area])[0]\n",
    "        location_string = str(loc[0]) + '-' + str(loc[1]) + '-' + str(loc[2]) + '-' + str(loc[3])\n",
    "        \n",
    "        #rootdir = '/Users/madsrindal/Desktop/Master project/SMAP/' + area + '-3years'\n",
    "        rootdir = '/Users/madsrindal/Desktop/Master project/SMAP/CentralAfrica'\n",
    "        \n",
    "        #processed_smap_storage_folder = '/Volumes/MadsDrive/Master/Processed Files/' + location_string\n",
    "        processed_smap_storage_folder = '/Users/madsrindal/Desktop/Intervals/' + location_string\n",
    "\n",
    "        #smap_df = get_smap_df_year(rootdir, desired_year, convert_time_hours=True\n",
    "        smap_df = get_smap_main(rootdir, years, range(13), range(100))\n",
    "\n",
    "        file_name = 'SMAP-all3Years-withQFs-[' + location_string + '].csv'\n",
    "        store_df_as_csv(smap_df, processed_smap_storage_folder, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INTERPOLATE SMAP DATAFRAME ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hours_after_jan_2019(old_hours, year):\n",
    "    if year == 2019:\n",
    "        return old_hours\n",
    "    elif year == 2020:\n",
    "        return old_hours + 24*365\n",
    "    else:\n",
    "        return old_hours + 24*365 + 24*366 # 366 fordi 2020 var skuddår\n",
    "\n",
    "\n",
    "def fix_hours_after_jan_2019(df):\n",
    "    df['hours_after_jan_2019'] = df.progress_apply(\n",
    "        lambda row: get_hours_after_jan_2019(row.hours_after_jan_2020, row.year), axis=1)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CYGNSS files...\n",
      "Fix hours after jan 2019...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3361360/3361360 [01:09<00:00, 48262.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading SMAP file...\n",
      "Creating interpolation function for soil moisture...\n",
      "Created smap_sm interpolation function in 100.9637701511383 seconds...\n",
      "\n",
      "Creating interpolation function for vegetation opacity...\n",
      "Created smap_vo interpolation function in 101.3816409111023 seconds...\n",
      "\n",
      "Creating interpolation function for surface roughness...\n",
      "Created smap_roughness interpolation function in 102.86946487426758 seconds...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if interpolate_dfs_boolean:\n",
    "\n",
    "    for area in locations_df['area']:\n",
    "        loc = list(locations_df['location'][locations_df['area'] == area])[0]\n",
    "        location_string = str(loc[0]) + '-' + str(loc[1]) + '-' + str(loc[2]) + '-' + str(loc[3])\n",
    "        interpolated_storage_folder = '/Users/madsrindal/Desktop/Intervals/' + location_string\n",
    "        \n",
    "        cygnss_df = pd.DataFrame()\n",
    "        print('Reading CYGNSS files...')\n",
    "        for year in years:\n",
    "            cygnss_df_tmp = pd.read_csv(interpolated_storage_folder + '/' + 'CYGNSS' + str(year) + '-withQFs-[' + location_string + '].csv')\n",
    "            cygnss_df_tmp['year'] = year\n",
    "            cygnss_df = cygnss_df.append(cygnss_df_tmp, ignore_index=True)\n",
    "        \n",
    "        print('Fix hours after jan 2019...')\n",
    "        cygnss_df = fix_hours_after_jan_2019(cygnss_df)\n",
    "        cygnss_df.drop('hours_after_jan_2020', inplace=True, axis=1)\n",
    "        \n",
    "        print('Reading SMAP file...')\n",
    "        #smap_df = pd.read_csv(interpolated_storage_folder + '/' + 'SMAP' + str(desired_year) + '-withQFs-[' + location_string + '].csv')\n",
    "        smap_df = pd.read_csv('/Users/madsrindal/Desktop/Intervals/' + location_string + '/SMAP-all3Years-withQFs-[' + location_string + '].csv')\n",
    "        smap_df = filter_smap_qfs(smap_df)\n",
    "\n",
    "        # Creating Interpolation Functions\n",
    "        print('Creating interpolation function for soil moisture...')\n",
    "        start_time = time.time()\n",
    "        smap_sm_func = interpolate_ml(smap_df, target_value='smap_sm')\n",
    "        print('Created smap_sm interpolation function in ' + str(time.time()-start_time) + ' seconds...\\n')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('Creating interpolation function for vegetation opacity...')\n",
    "        smap_vo_func = interpolate_ml(smap_df, target_value='vegetation_opacity')\n",
    "        print('Created smap_vo interpolation function in ' + str(time.time()-start_time) + ' seconds...\\n')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('Creating interpolation function for surface roughness...')\n",
    "        smap_roughness_func = interpolate_ml(smap_df, target_value='surface_roughness')\n",
    "        print('Created smap_roughness interpolation function in ' + str(time.time()-start_time) + ' seconds...\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick fix ###\n",
    "\n",
    "best_loc = [-10, 27, -10.5, 27.5]\n",
    "forest_loc = [-10, 23.5, -10.5, 24]\n",
    "worst_loc = [-8, 27.5, -8.5, 28]\n",
    "\n",
    "# Filter CYGNSS QFs\n",
    "cygnss_df = filter_cygnss_qf(cygnss_df)\n",
    "\n",
    "# Select smaller area\n",
    "best_cell_df = filter_location(cygnss_df, best_loc)\n",
    "forest_cell_df = filter_location(cygnss_df, forest_loc)\n",
    "worst_cell_df = filter_location(cygnss_df, worst_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating SMAP soil moisture values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30133/30133 [01:38<00:00, 305.88it/s]\n",
      "100%|██████████| 29435/29435 [00:58<00:00, 499.28it/s] \n",
      "100%|██████████| 30174/30174 [02:03<00:00, 243.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated smap_sm data frame in 281.4665811061859 seconds...\n",
      "\n",
      "Interpolating SMAP vegetation opacity values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30133/30133 [01:37<00:00, 307.82it/s]\n",
      "100%|██████████| 29435/29435 [00:58<00:00, 499.23it/s] \n",
      "100%|██████████| 30174/30174 [01:59<00:00, 253.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated smap_vo data frame in 275.9295828342438 seconds...\n",
      "\n",
      "Interpolating SMAP surface roughness values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30133/30133 [01:38<00:00, 307.47it/s]\n",
      "100%|██████████| 29435/29435 [00:59<00:00, 497.51it/s] \n",
      "100%|██████████| 30174/30174 [01:57<00:00, 256.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated smap_roughness data frame in 275.00737500190735 seconds...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpolating Data Frame\n",
    "print('Interpolating SMAP soil moisture values...')\n",
    "start_time = time.time()\n",
    "best_interpolated_df = interpolate_smap_df(best_cell_df, smap_sm_func, 'smap_sm')\n",
    "worst_interpolated_df = interpolate_smap_df(worst_cell_df, smap_sm_func, 'smap_sm')\n",
    "forest_interpolated_df = interpolate_smap_df(forest_cell_df, smap_sm_func, 'smap_sm')\n",
    "print('Interpolated smap_sm data frame in ' + str(time.time()-start_time) + ' seconds...\\n')\n",
    "\n",
    "print('Interpolating SMAP vegetation opacity values...')\n",
    "start_time = time.time()\n",
    "best_interpolated_df = interpolate_smap_df(best_interpolated_df, smap_vo_func, 'smap_vo')\n",
    "worst_interpolated_df = interpolate_smap_df(worst_interpolated_df, smap_vo_func, 'smap_vo')\n",
    "forest_interpolated_df = interpolate_smap_df(forest_interpolated_df, smap_vo_func, 'smap_vo')\n",
    "print('Interpolated smap_vo data frame in ' + str(time.time()-start_time) + ' seconds...\\n')\n",
    "\n",
    "print('Interpolating SMAP surface roughness values...')\n",
    "start_time = time.time()\n",
    "best_interpolated_df = interpolate_smap_df(best_interpolated_df, smap_roughness_func, 'smap_surface_roughness')\n",
    "worst_interpolated_df = interpolate_smap_df(worst_interpolated_df, smap_roughness_func, 'smap_surface_roughness')\n",
    "forest_interpolated_df = interpolate_smap_df(forest_interpolated_df, smap_roughness_func, 'smap_surface_roughness')\n",
    "print('Interpolated smap_roughness data frame in ' + str(time.time()-start_time) + ' seconds...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save quick fix tables ##\n",
    "file_name_best = 'BestInterpolatedDF-Congo-' + str(best_loc) + '.csv'\n",
    "file_name_worst = 'WorstInterpolatedDF-Congo-' + str(worst_loc) + '.csv'\n",
    "file_name_forest = 'ForestInterpolatedDF-Congo' + str(forest_loc) + '.csv'\n",
    "\n",
    "store_df_as_csv(best_interpolated_df, interpolated_storage_folder, file_name_best)\n",
    "store_df_as_csv(worst_interpolated_df, interpolated_storage_folder, file_name_worst)\n",
    "store_df_as_csv(forest_interpolated_df, interpolated_storage_folder, file_name_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'InterpolatedDF-withCYGNSSQFs-[' + location_string + '].csv'\n",
    "store_df_as_csv(interpolated_df, interpolated_storage_folder, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = interpolated_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = test_df.shape[0]\n",
    "filtered_df = filter_nan_smap_sm(test_df)\n",
    "filtered_df = filter_nan_smap_vo(filtered_df)\n",
    "filtered_df = filter_nan_smap_sr(filtered_df)\n",
    "after = filtered_df.shape[0]\n",
    "\n",
    "print('Removed ' + str(before-after) + ' rows containing nan values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANNET ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('8: ', str(bin(8)).split('b')[1].zfill(16))\n",
    "print('9: ', str(bin(9)).split('b')[1].zfill(16))\n",
    "print('13: ', str(bin(13)).split('b')[1].zfill(16))\n",
    "\n",
    "print('2: ', str(bin(2)).split('b')[1].zfill(16))\n",
    "\n",
    "print(0 % 2)\n",
    "\n",
    "print(type(bin(8)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TIME SERIES - FLOOD DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "from matplotlib import pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import xarray as xr\n",
    "import random\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from itertools import product\n",
    "from scipy.signal import savgol_filter\n",
    "tqdm.pandas()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "from random import random\n",
    "\n",
    "\n",
    "# Main time series for loop\n",
    "def time_series_analysis(cygnss_df, smap_df, area, moving_window=2, interval=10, use_median=True, \n",
    "                         plot=True, area_name=None, save=None, use_smoothening=False, sigma=None,\n",
    "                         error_bar_scale=10):\n",
    "    \n",
    "    cygnss_df = filter_cygnss_df(cygnss_df, area)\n",
    "    smap_df = filter_cygnss_df(smap_df, area)\n",
    "    \n",
    "    first_day = cygnss_df['day_of_year'].min()\n",
    "    last_day = cygnss_df['day_of_year'].max()\n",
    "    \n",
    "    surface_ref = {}\n",
    "    surface_ref_std = {}\n",
    "    soil_moisture = {}\n",
    "    soil_moisture_std = {}\n",
    "    \n",
    "    for day in np.arange(first_day, last_day - moving_window, moving_window):\n",
    "        # Filter CYGNSS\n",
    "        current_cygnss = filter_cygnss_day(cygnss_df, day, day + interval)\n",
    "        if use_smoothening:\n",
    "            current_cygnss = grid_box(current_cygnss, 'sr', True)\n",
    "            current_cygnss = smoothening(current_cygnss, area, sigma, 'sr')\n",
    "        \n",
    "        # Filter SMAP\n",
    "        current_smap = filter_smap_day(smap_df, day*24, (day + interval)*24)\n",
    "        \n",
    "        if use_median:\n",
    "            current_sr = current_cygnss['sr'].median()\n",
    "            current_sr_std = current_cygnss['sr'].mad()\n",
    "        else:\n",
    "            current_sr = current_cygnss['sr'].mean()\n",
    "            current_sr_std = current_cygnss['sr'].std()\n",
    "        \n",
    "        if len(current_smap) > 0:\n",
    "            current_sm = current_smap['smap_sm'].mean()\n",
    "            current_sm_std = current_smap['smap_sm'].std()\n",
    "        else:\n",
    "            # print('EMPTY SMAP from day:', day, 'to day:', day + interval)\n",
    "            current_sm = np.mean(list(soil_moisture.values()))\n",
    "            current_sm_std = 0\n",
    "        \n",
    "        surface_ref[day] = current_sr\n",
    "        surface_ref_std[day] = current_sr_std\n",
    "        soil_moisture[day] = current_sm\n",
    "        soil_moisture_std[day] = current_sm_std\n",
    "    \n",
    "    if plot:\n",
    "        \n",
    "        error_bar_top = []\n",
    "        error_bar_bottom = []\n",
    "        \n",
    "        for i in range(len(list(surface_ref.values()))):\n",
    "            error_bar_top.append(list(surface_ref.values())[i] + list(surface_ref_std.values())[i]/error_bar_scale)\n",
    "            error_bar_bottom.append(list(surface_ref.values())[i] - list(surface_ref_std.values())[i]/error_bar_scale)\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.axes()\n",
    "        ax.grid()\n",
    "        \n",
    "        ax.plot(list(surface_ref.keys()), list(surface_ref.values()), color=\"red\", marker=\"o\")\n",
    "        ax.fill_between(list(surface_ref_std.keys()), error_bar_top, error_bar_bottom, alpha=.2, linewidth=0, color='red')\n",
    "        label_size = 20\n",
    "        ax.set_xlabel(\"Day after 1st of Jan 2019\", fontsize=label_size)\n",
    "        ax.set_ylabel(\"SR [dB]\", color=\"red\", fontsize=label_size)\n",
    "        \n",
    "\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.grid(False)\n",
    "        ax2.plot(list(soil_moisture.keys()), list(soil_moisture.values()), color=\"blue\", marker=\"o\")\n",
    "        ax2.set_ylabel(\"SM [cm^3/cm^3]\", color=\"blue\", fontsize=label_size)\n",
    "        ax.legend(['Surface Reflectivity (SR)', 'std SR/' + str(error_bar_scale)], loc=2)\n",
    "        ax2.legend(['Soil Moisture (SM)'], loc=3)\n",
    "        \n",
    "        if area_name is not None:\n",
    "            plt.title(area_name + \" Correlation: \" + str(round(pd.Series(surface_ref.values()).corr(pd.Series(soil_moisture.values())), 3)), fontsize=22)\n",
    "            if save is not None:\n",
    "                plt.savefig('/Users/vegardhaneberg/Desktop/Plots Master/Time Series/' + str(area_name) + '.png', format='png')\n",
    "        else:\n",
    "            plt.title(\"Correlation: \" + str(round(pd.Series(surface_ref.values()).corr(pd.Series(soil_moisture.values())), 3)), fontsize=18)\n",
    "            if save is not None:\n",
    "                random_num = lambda: random.randint(0, 255)\n",
    "                random_name = '%02X%02X%02X' % (random_num(), random_num(), random_num())\n",
    "                plt.savefig('/Users/vegardhaneberg/Desktop/Plots Master/Time Series/Without Name' + random_name + '.png', format='png')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    return surface_ref, surface_ref_std, soil_moisture, soil_moisture_std\n",
    "\n",
    "\n",
    "def filter_cygnss_df(df: pd.DataFrame, area: dict) -> pd.DataFrame:\n",
    "    if 'sp_lat' in df.columns:\n",
    "        new_df = df[df['sp_lat'] <= area['north']]\n",
    "        new_df = new_df[new_df['sp_lat'] >= area['south']]\n",
    "        new_df = new_df[new_df['sp_lon'] >= area['west']]\n",
    "        new_df = new_df[new_df['sp_lon'] <= area['east']]\n",
    "    else:\n",
    "        new_df = df[df['lat'] <= area['north']]\n",
    "        new_df = new_df[new_df['lat'] >= area['south']]\n",
    "        new_df = new_df[new_df['long'] >= area['west']]\n",
    "        new_df = new_df[new_df['long'] <= area['east']]\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def filter_cygnss_day(df, start_day, end_day):\n",
    "    filtered_df = df[df['day_of_year'] >= start_day]\n",
    "    filtered_df = filtered_df[filtered_df['day_of_year'] < end_day]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def filter_smap_day(df, start_hour, end_hour):\n",
    "    filtered_df = df[df['time'] >= start_hour]\n",
    "    filtered_df = filtered_df[filtered_df['time'] < end_hour]\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {'north': 35, 'west': 49.5, 'south': 30, 'east': 54.5}\n",
    "step_size = 2\n",
    "num_days_avg = 2\n",
    "use_median = False\n",
    "plot_time_series = True\n",
    "plot_name = 'Plot Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyg_df_ts = pd.read_csv('/Users/madsrindal/Desktop/Intervals/35-49.5-30-54.5/CYGNSS2020-withQFs-[35-49.5-30-54.5].csv').rename(columns={'sp_lat': 'lat', 'sp_lon':'long'})\n",
    "smap_df_ts = pd.read_csv('/Users/madsrindal/Desktop/Intervals/35-49.5-30-54.5/SMAP2020-withQFs-[35-49.5-30-54.5].csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyg_df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_analysis(cyg_df_ts, smap_df_ts, area_dict, step_size, num_days_avg, use_median, plot_time_series, plot_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {339: 1, 340: 2}\n",
    "print(list(a.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_nearest(x, a):\n",
    "    return round(x / a) * a\n",
    "\n",
    "\n",
    "def grid_box(df, resolution, target_value='sr', use_median=False):\n",
    "    #df['lat'] = df['lat'].apply(lambda x: round(round_nearest(x, resolution), 1))\n",
    "    df['lat'] = df['lat'].apply(lambda x: round_nearest(x, resolution), 1)\n",
    "    #df['long'] = df['long'].apply(lambda x: round(round_nearest(x, resolution), 1))\n",
    "    df['long'] = df['long'].apply(lambda x: round_nearest(x, resolution), 1)\n",
    "\n",
    "    if use_median:\n",
    "        df = df.groupby(['long', 'lat'], as_index=False)[target_value].median()\n",
    "    else:\n",
    "        df = df.groupby(['long', 'lat'], as_index=False)[target_value].mean()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smap(path: str, printing=False):\n",
    "\n",
    "    ds = nc.Dataset(path)\n",
    "    sm = ds['Soil_Moisture_Retrieval_Data_AM']\n",
    "    \n",
    "    print(sm['latitude'])\n",
    "    \n",
    "    lats = np.array(sm['latitude']).flatten()\n",
    "    lons = np.array(sm['longitude']).flatten()\n",
    "    times = np.array(sm['tb_time_utc']).flatten()\n",
    "    sms = np.array(sm['soil_moisture']).flatten()\n",
    "    qfs = np.array(sm['retrieval_qual_flag']).flatten()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['lat'] = lats\n",
    "    df['long'] = lons\n",
    "    df['time'] = times\n",
    "    df['smap_sm'] = sms\n",
    "    df['retrieval_qfs'] = qfs\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/madsrindal/Downloads/SMAP_L3_SM_P_20190102_R18290_001_HEGOUT.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df = get_smap(path)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_code(path: str):\n",
    "\n",
    "    ds = nc.Dataset(path)\n",
    "    sm = ds['Soil_Moisture_Retrieval_Data_AM']\n",
    "\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    moistures = []\n",
    "    time_values = []\n",
    "    qfs = []\n",
    "\n",
    "    for lat in tqdm(range(len(sm['latitude']))):\n",
    "        for long in range(len(sm['longitude'][lat])):\n",
    "            latitudes.append(sm['latitude'][lat][long])\n",
    "            longitudes.append(sm['longitude'][lat][long])\n",
    "            moistures.append(sm['soil_moisture'][lat][long])\n",
    "            time_values.append(sm['tb_time_utc'][lat][long])\n",
    "            qfs.append(sm['retrieval_qual_flag'][lat][long])\n",
    "\n",
    "    return latitudes, longitudes, moistures, time_values, qfs\n",
    "\n",
    "\n",
    "def filter_smap_qfs(smap_df):\n",
    "    return smap_df.loc[(smap_df['retrieval_qfs'] == 0) | (smap_df['retrieval_qfs'] == 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes, longitudes, moistures, times, qfs = test_code(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(latitudes))\n",
    "print(len(longitudes))\n",
    "print(len(moistures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = '/Users/madsrindal/Downloads/SMAP_L3_SM_P_20190102_R18290_001_HEGOUT.nc'\n",
    "ds = nc.Dataset(pth)\n",
    "sm = ds['Soil_Moisture_Retrieval_Data_AM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sm['soil_moisture'][2][0]))\n",
    "\n",
    "lats = np.array(sm['latitude']).flatten()\n",
    "lons = np.array(sm['longitude']).flatten()\n",
    "sms = np.array(sm['soil_moisture']).flatten()\n",
    "qs = np.array(sm['retrieval_qual_flag']).flatten()\n",
    "\n",
    "print(type(list(sms)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_fill_value(value):\n",
    "    try:\n",
    "        return np.float32(value)\n",
    "    except:\n",
    "        return -9999.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conv_fill_value(12)\n",
    "print(type(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df['lat'] = latitudes\n",
    "test_df['long'] = longitudes\n",
    "test_df['smap_sm'] = moistures\n",
    "\n",
    "#test_df['smap_sm'] = test_df['smap_sm'].progress_apply(lambda x: conv_fill_value(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['time', 'retrieval_qfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads_df = df.copy()\n",
    "vegard_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads_df = mads_df[(mads_df['lat'] != -9999.0) & (mads_df['long'] != -9999.0)]\n",
    "vegard_df = vegard_df[(vegard_df['lat'] != -9999.0) & (vegard_df['long'] != -9999.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads_df['smap_sm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegard_df['smap_sm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegard_df['smap_sm'] = vegard_df['smap_sm'].apply(lambda x: str(x))\n",
    "vegard_df['smap_sm'] = vegard_df['smap_sm'].apply(lambda x: -9999.0 if x=='--' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegard_df['smap_sm'] = vegard_df['smap_sm'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegard_df = vegard_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegard_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads_df = mads_df[mads_df['smap_sm'] != -9999.0]\n",
    "print(mads_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads_df_list = mads_df.index.tolist()\n",
    "vegard_df_list = vegard_df.index.tolist()\n",
    "\n",
    "print('Mads_df_list: ', len(mads_df_list))\n",
    "print('Vegard_df_list: ', len(vegard_df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_list = list(set(mads_df_list) - set(vegard_df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(main_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num in main_list:\n",
    "    print(moistures[num])\n",
    "    print(sms[num])\n",
    "    print('x'*50)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[8269]['smap_sm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[8269]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_ticks(lat_values, long_values):\n",
    "    min_lat = min(lat_values)\n",
    "    max_lat = max(lat_values)\n",
    "    min_long = min(long_values)\n",
    "    max_long = max(long_values)\n",
    "    \n",
    "    lat_step_size = (max_lat - min_lat) / 3\n",
    "    long_step_size = (max_long - min_long) / 3\n",
    "    \n",
    "    long_list = [min_long, min_long + long_step_size, min_long + 2 * long_step_size, max_long]\n",
    "    lat_list = [min_lat, min_lat + lat_step_size, min_lat + 2 * lat_step_size, max_lat]\n",
    "    \n",
    "    # Rounding to two decimals\n",
    "    long_list = [round(num, 2) for num in long_list]\n",
    "    lat_list = [round(num, 2) for num in lat_list]\n",
    "    \n",
    "    return lat_list, long_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def universal_plot(df, target_value='swvl1', title=None, bar_title=None, vmin=None, vmax=None, save=None, dot_size=0.5, std=False, fig_size=None, regions=None, region_colors=None, region_names=None):\n",
    "    \n",
    "    if fig_size is not None:\n",
    "        plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    else:\n",
    "        plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "    \n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    lat_list, long_list = get_plot_ticks(df['lat'], df['long'])\n",
    "    ax.set_xticks(long_list, crs=ccrs.PlateCarree())\n",
    "    ax.set_yticks(lat_list, crs=ccrs.PlateCarree())\n",
    "    ax.xaxis.set_major_formatter(LongitudeFormatter())\n",
    "    ax.yaxis.set_major_formatter(LatitudeFormatter())\n",
    "    \n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    \n",
    "    if std:\n",
    "        cmap = 'Greys'\n",
    "    else:\n",
    "        cmap = 'Spectral'\n",
    "        \n",
    "    if vmin is not None:\n",
    "        plt.scatter(df['long'], df['lat'], c=list(df[target_value]), s=dot_size, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    else:\n",
    "        plt.scatter(df['long'], df['lat'], c=list(df[target_value]), s=dot_size, cmap=cmap)\n",
    "    \n",
    "    bar = plt.colorbar(shrink=0.7)\n",
    "    bar.ax.tick_params(labelsize=15)\n",
    "    if bar_title is not None:\n",
    "        bar.ax.set_title(bar_title, fontsize=18)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=20, fontweight='book')\n",
    "    \n",
    "    if regions is not None:\n",
    "        legend_elements = []\n",
    "        \n",
    "        for i in range(len(regions)):\n",
    "            ax.add_patch(regions[i])\n",
    "            legend_elements.append(Line2D([0], [0], color=region_colors[i], lw=3, label=region_names[i]))\n",
    "\n",
    "        ax.legend(handles=legend_elements, loc='upper right', fontsize=16)\n",
    "        \n",
    "    plt.xlabel('Longitude', fontsize=18)\n",
    "    plt.ylabel('Latitude', fontsize=18)\n",
    "    \n",
    "    if save is not None:\n",
    "        plt.savefig(save, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "from progressbar import progressbar\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "from matplotlib import pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import xarray as xr\n",
    "import random\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from itertools import product\n",
    "from scipy.signal import savgol_filter\n",
    "tqdm.pandas()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "from random import random\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_plot(df, 'smap_sm', fig_size=(14,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = df['smap_sm'].max()\n",
    "min_value = df['smap_sm'].min()\n",
    "mean_value = df['smap_sm'].mean()\n",
    "\n",
    "print('Max: ', max_value)\n",
    "print('Min: ', min_value)\n",
    "print('Snitt: ', mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = test_df['smap_sm'].max()\n",
    "min_value = test_df['smap_sm'].min()\n",
    "mean_value = test_df['smap_sm'].mean()\n",
    "\n",
    "print('Max: ', max_value)\n",
    "print('Min: ', min_value)\n",
    "print('Snitt: ', mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
